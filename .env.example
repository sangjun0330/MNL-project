NEXT_ALLOWED_DEV_ORIGINS=http://localhost:3000
NEXT_PUBLIC_SITE_URL=https://rnest.kr

# AI Med Safety
# 권장 범위: 3600~7000 (응답 품질 우선)
OPENAI_MODEL=gpt-5.1
OPENAI_MED_SAFETY_MODEL=gpt-5.1
OPENAI_MED_SAFETY_MAX_OUTPUT_TOKENS=7000
# AI 검색기 모델 고정(gpt-5.1 단일 사용)
OPENAI_MED_SAFETY_FALLBACK_MODELS=gpt-5.1
# 네트워크 재시도(권장 1~3)
OPENAI_MED_SAFETY_NETWORK_RETRIES=2
OPENAI_MED_SAFETY_NETWORK_RETRY_BASE_MS=700
# 선택: 기본/대체 API 베이스 URL 목록(쉼표 구분)
# OPENAI_MED_SAFETY_BASE_URLS=https://api.openai.com/v1
# OpenAI 상위 호출 단건 타임아웃(ms)
OPENAI_MED_SAFETY_UPSTREAM_TIMEOUT_MS=120000
# 전체 분석 최대 대기시간(ms): 초과 시 fallback
OPENAI_MED_SAFETY_TOTAL_BUDGET_MS=420000
# 분석 요청 타임아웃(ms)
OPENAI_MED_SAFETY_TIMEOUT_MS=600000

# Supabase (Auth + DB)
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-supabase-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-supabase-service-role-key
SUPABASE_DATABASE_URL=postgresql://user:pass@host:5432/postgres

# TossPayments
NEXT_PUBLIC_TOSS_CLIENT_KEY=your-toss-client-key
TOSS_SECRET_KEY=your-toss-secret-key
TOSS_API_ACCEPT_LANGUAGE=ko-KR
# Optional (test key only): forces test error simulation code from Toss docs
TOSS_TEST_CODE=
# Optional: verify incoming webhook with query/header token
TOSS_WEBHOOK_TOKEN=
# Optional: restrict webhook source IP/CIDR list (comma-separated)
TOSS_WEBHOOK_IP_ALLOWLIST=

# Handoff Tool (local-only pipeline)
NEXT_PUBLIC_HANDOFF_ENABLED=true
# Execution mode: local_only | hybrid_opt_in
# local_only(권장): 원문/구조화 모두 온디바이스 처리, remote sync 강제 비활성
# hybrid_opt_in: 향후 비식별 원격 경로를 정책/동의 기반으로 열어둘 때만 사용
NEXT_PUBLIC_HANDOFF_EXECUTION_MODE=local_only
# NOTE: local_only에서는 true로 설정해도 정책에서 false로 강제됩니다.
NEXT_PUBLIC_HANDOFF_REMOTE_SYNC_ENABLED=false
# Privacy profile: strict | standard
# strict: web_speech 차단 + same-origin wasm URL 강제 + 인증 요구
NEXT_PUBLIC_HANDOFF_PRIVACY_PROFILE=strict
NEXT_PUBLIC_HANDOFF_REQUIRE_AUTH=true
NEXT_PUBLIC_HANDOFF_LOCAL_ASR_ENABLED=true
NEXT_PUBLIC_HANDOFF_EVIDENCE_ENABLED=true
# Web ASR provider: manual | web_speech | wasm_local
# NOTE: local_only/strict에서는 web_speech가 정책으로 차단될 수 있습니다.
NEXT_PUBLIC_HANDOFF_ASR_PROVIDER=wasm_local
NEXT_PUBLIC_HANDOFF_WEB_AUDIO_CAPTURE_ENABLED=true
# WASM on-device ASR (for handoff_asr_provider=wasm_local)
NEXT_PUBLIC_HANDOFF_WASM_ASR_ENABLED=true
NEXT_PUBLIC_HANDOFF_WASM_ASR_WORKER_URL=/workers/handoff-whisper.worker.js
# Optional: model/runtime URLs used by worker/plugin.
NEXT_PUBLIC_HANDOFF_WASM_ASR_MODEL_URL=
NEXT_PUBLIC_HANDOFF_WASM_ASR_RUNTIME_URL=/runtime/whisper-runtime.js
# Whisper backend engine: worker_runtime | transformers_whisper
NEXT_PUBLIC_HANDOFF_WASM_ASR_ENGINE=transformers_whisper
# Default Whisper model (requested): Whisper Small
NEXT_PUBLIC_HANDOFF_WASM_ASR_MODEL_ID=openai/whisper-small
# Device hint: auto | webgpu | wasm
NEXT_PUBLIC_HANDOFF_WASM_ASR_DEVICE=auto
# Quantization hint for transformers backend
NEXT_PUBLIC_HANDOFF_WASM_ASR_DTYPE=q8
# Optional CDN override for transformers runtime (default: jsdelivr)
# NEXT_PUBLIC_HANDOFF_WASM_ASR_TRANSFORMERS_URL=https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.8.1/dist/transformers.min.js
# VAD(무음 제거) 옵션
NEXT_PUBLIC_HANDOFF_VAD_ENABLED=true
NEXT_PUBLIC_HANDOFF_VAD_MIN_SPEECH_RATIO=0.05
NEXT_PUBLIC_HANDOFF_VAD_MIN_SEGMENT_MS=180
NEXT_PUBLIC_HANDOFF_VAD_THRESHOLD=0.012
# WebLLM 기반 문장 다듬기(옵션, window.__RNEST_WEBLLM_REFINE__ 어댑터 필요)
NEXT_PUBLIC_HANDOFF_WEBLLM_REFINE_ENABLED=true
# WebLLM is required for masked handoff output (no heuristic fallback in UI flow)
NEXT_PUBLIC_HANDOFF_WEBLLM_REQUIRED=true
# Use MLC WebLLM backend (forced when WEBLLM_REQUIRED=true)
NEXT_PUBLIC_HANDOFF_WEBLLM_USE_MLC=true
# Default WebLLM model (requested): Qwen2.5-3B-q4f16
NEXT_PUBLIC_HANDOFF_WEBLLM_MODEL_ID=Qwen2.5-3B-Instruct-q4f16_1-MLC
# WebLLM module URL (served locally by prebuild script)
NEXT_PUBLIC_HANDOFF_WEBLLM_MODULE_URL=/runtime/vendor/web-llm/index.js
# Optional cache-bust token to force fresh WebLLM runtime fetch after deploy/CSP changes
NEXT_PUBLIC_HANDOFF_WEBLLM_MODULE_VERSION=Qwen2.5-3B-Instruct-q4f16_1-MLC
# GPU 미지원 브라우저에서 WebLLM 실패 시 WASM LLM 백업 경로(Transformers.js)
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_ENABLED=true
# 기본값은 UI 프리징 방지를 위해 경량 모델 사용 (필요 시만 더 큰 모델로 변경)
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_MODEL_ID=Xenova/distilgpt2
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_DEVICE=wasm
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_DTYPE=q8
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_MAX_NEW_TOKENS=220
# local /models/* 선조회(404) 비활성화: 필요 시에만 true
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_ALLOW_LOCAL_MODELS=false
# wasm 단일 스레드에서 무거운 모델로 인한 멈춤 방지 가드(기본 false 유지)
NEXT_PUBLIC_HANDOFF_WEBLLM_WASM_FALLBACK_ALLOW_HEAVY_MODEL=false
# 기본값(false): 필수 모드에서는 MLC(WebGPU) 경로를 우선 시도하고 실패 시 대체 경로로 전환
NEXT_PUBLIC_HANDOFF_WEBLLM_SKIP_MLC_WEBGPU=false
# Optional: WebLLM/ASR와 동일한 transformers runtime CDN 재사용
# NEXT_PUBLIC_HANDOFF_WEBLLM_TRANSFORMERS_URL=https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.8.1/dist/transformers.min.js
# Rewrite MLC model_lib downloads away from raw.githubusercontent.com for CSP compatibility
NEXT_PUBLIC_HANDOFF_WEBLLM_MODEL_LIB_BASE_URL=https://cdn.jsdelivr.net/gh/mlc-ai/binary-mlc-llm-libs@main/
# WebLLM refine 백엔드 URL(상대경로/동일출처 권장)
NEXT_PUBLIC_HANDOFF_WEBLLM_BACKEND_URL=/runtime/webllm-refine-backend.js
# WebLLM refine 어댑터 URL(상대경로/동일출처 권장)
NEXT_PUBLIC_HANDOFF_WEBLLM_ADAPTER_URL=/runtime/webllm-refine-adapter.js
# Raw/evidence를 디스크에 남기지 않는 메모리 전용 모드(권장)
NEXT_PUBLIC_HANDOFF_LIVE_MEMORY_ONLY=true
# Structured 저장 TTL (시간 단위): 권장 24 (옵션 168=7일, 720=30일)
NEXT_PUBLIC_HANDOFF_STRUCTURED_TTL_HOURS=24
